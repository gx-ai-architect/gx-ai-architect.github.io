<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Personal website of Guangxuan (GX) Xu - Principal Research Scientist">
    <title>Guangxuan (GX) Xu - AI/ML Research Scientist</title>
    <link rel="stylesheet" href="style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <nav class="navbar">
        <div class="container">
            <div class="nav-content">
                <a href="#" class="logo">Guangxuan (GX) Xu</a>
                <ul class="nav-links">
                    <li><a href="#about">About</a></li>
                    <li><a href="#experience">Experience</a></li>
                    <li><a href="#projects">Projects</a></li>
                    <li><a href="#skills">Skills</a></li>
                    <li><a href="#contact">Contact</a></li>
                </ul>
            </div>
        </div>
    </nav>

    <main>
        <section id="hero" class="hero">
            <div class="container">
                <div class="hero-content">
                    <img src="gx_new_profile.png" alt="Profile Picture" class="profile-pic" id="profile-pic">
                    <h1>Guangxuan (GX) Xu</h1>
                    <p class="title">Principal Research Scientist | AI/ML Researcher</p>
                    <p class="location">Boston, MA</p>
                    <div class="social-links">
                        <a href="https://github.com/gx-ai-architect" target="_blank" rel="noopener">GitHub</a>
                        <a href="https://www.linkedin.com/in/gxxu/" target="_blank" rel="noopener">LinkedIn</a>
                        <a href="https://scholar.google.com/citations?user=ohsEWqsAAAAJ&hl=en" target="_blank" rel="noopener">Google Scholar</a>
                        <a href="mailto:gxu21@cs.ucla.edu">Email</a>
                    </div>
                </div>
            </div>
        </section>

        <section id="about" class="section">
            <div class="container">
                <h2>About</h2>
                <p class="about-text">
                    I'm an AI optimist—not the naive kind, but the kind who's seen code generate code, agents debug themselves, and research accelerate at unprecedented rates. Yes, the doomers might have a point about existential risks, but I'd rather spend my energy building guardrails at full speed than philosophizing at a standstill. We're at a rare moment where researchers can actually shape how this technology unfolds, and I'm not about to waste it. My research focuses on test-time scaling and test-time finetuning. I believe in building agents with sanity—simple agents that work. I love open source AI projects, especially agent frameworks—testing things, learning things, and building things with AI agents, for AI agents, to better manage AI agents.
                </p>
                <p class="about-text">
                    Currently, I'm a Principal Research Scientist at Red Hat, working on enterprise AI agents and post-training techniques for LLMs. I hold an MS in Computer Science from UCLA and a BA from Wesleyan University. My work spans publications at NeurIPS, ICLR, ICML, ACL, and EMNLP - see my <a href="https://scholar.google.com/citations?user=ohsEWqsAAAAJ&hl=en" target="_blank" rel="noopener">Google Scholar</a> for more details.
                </p>
            </div>
        </section>

        <section id="news" class="section section-alt">
            <div class="container">
                <h2>Recent Updates</h2>
                <ul class="news-list">
                    <li>
                        <span class="date">2025</span>
                        <span class="news-item">Paper accepted to <a href="https://ishapuri.github.io/assets/pdfs/particle_filtering_project_arxiv.pdf" target="_blank" rel="noopener">NeurIPS 2025</a>: "Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods"</span>
                    </li>
                    <li>
                        <span class="date">2025</span>
                        <span class="news-item">New preprint: <a href="https://arxiv.org/pdf/2510.05825" target="_blank" rel="noopener">"Entropic Particle-Filtering for Inference-Time Scaling"</a> on arXiv</span>
                    </li>
                    <li>
                        <span class="date">2025</span>
                        <span class="news-item">New preprint: <a href="https://arxiv.org/abs/2411.02481" target="_blank" rel="noopener">"Dr. SoW: Density Ratio of Strong-over-Weak LLMs for Preference Tuning"</a> on arXiv</span>
                    </li>
                    <li>
                        <span class="date">2025</span>
                        <span class="news-item">Paper accepted to <a href="https://openreview.net/pdf?id=eENHKMTOfW" target="_blank" rel="noopener">ICLR 2025</a>: "Unveiling the Secret Recipe: A Guide for Supervised Fine-Tuning Small LLMs"</span>
                    </li>
                    <li>
                        <span class="date">2024</span>
                        <span class="news-item">Paper accepted to <a href="https://arxiv.org/abs/2402.02479" target="_blank" rel="noopener">ICML 2024</a>: "BRAIn: Bayesian Reward-conditioned Amortized Inference for NLG from Feedback"</span>
                    </li>
                    <li>
                        <span class="date">2024</span>
                        <span class="news-item">Paper accepted to <a href="https://aclanthology.org/2024.findings-acl.10.pdf" target="_blank" rel="noopener">ACL 2024 Findings</a>: "A Grounded Preference Model for LLM Alignment"</span>
                    </li>
                    <li>
                        <span class="date">2023</span>
                        <span class="news-item">Paper accepted to <a href="https://arxiv.org/abs/2305.16641" target="_blank" rel="noopener">ACL 2023</a>: "Are Fairy Tales Fair? Analyzing Gender Bias in Temporal Narrative Event Chains"</span>
                    </li>
                    <li>
                        <span class="date">2022</span>
                        <span class="news-item"><a href="https://arxiv.org/abs/2104.14795" target="_blank" rel="noopener">Best Paper Award</a> at AAAI 2021 for work on mitigating political bias in language models</span>
                    </li>
                </ul>
            </div>
        </section>

        <section id="experience" class="section">
            <div class="container">
                <h2>Experience</h2>
                <div class="experience-item">
                    <h3>Principal Research Scientist</h3>
                    <p class="company">Red Hat, Inc. | Boston, MA</p>
                    <p class="period">November 2024 - Present</p>
                    <ul class="description">
                        <li>Led design and development of enterprise-grade agent system within IBM's WatsonX Orchestrate. Core contributor and architect for agent evaluation and feedback loop.</li>
                        <li>Spearheaded integration of inference-time scaling and reward modeling techniques into production stack, enabling agents to learn from evaluation results and improve through compute scaling and reinforcement learning.</li>
                        <li>Led preference training for RHELAI – a flagship AI product for IBM and Red Hat. Invented Dr.SoW, a novel method for automatic preference annotation that significantly reduces human labeling cost.</li>
                        <li>Co-invented particle-based sampling algorithm for efficient LLM inference-time compute scaling, achieving state-of-the-art accuracy-compute tradeoffs.</li>
                    </ul>
                </div>
                <div class="experience-item">
                    <h3>Research Engineer</h3>
                    <p class="company">IBM Research, Thomas J. Watson Research Center | Yorktown Heights, NY</p>
                    <p class="period">January 2023 - November 2024</p>
                    <ul class="description">
                        <li>Led development and release of Merlinite-7B-PT, the first preference-aligned LLM from the InstructLab project, achieving alignment performance competitive with GPT-3.5 and Claude-v1 using only 48k AI-generated preference pairs.</li>
                        <li>Developed reward modeling techniques with document-grounded feedback to improve faithfulness and reasoning in QA and RAG tasks.</li>
                        <li>Co-developed Bayesian framework for preference alignment included in technical reports of Nvidia's Nemotron series and IBM's Granite models.</li>
                    </ul>
                </div>
                <div class="experience-item">
                    <h3>AI Research Intern</h3>
                    <p class="company">IBM Research, Thomas J. Watson Research Center | Yorktown Heights, NY</p>
                    <p class="period">June 2022 - September 2022</p>
                    <ul class="description">
                        <li>Improved training of IBM's SOTA BART-based AMR parser with dynamic MLM and features creation, reducing training time by 20% and memory requirement by 130G.</li>
                        <li>Led development of first jointly-trained text and AMR-graph model with novel constrained decoding scheme, approaching SOTA Smatch scores.</li>
                    </ul>
                </div>
                <div class="experience-item">
                    <h3>Graduate Research Assistant</h3>
                    <p class="company">PLUS Lab, UCLA | Los Angeles, CA</p>
                    <p class="period">September 2021 - Present</p>
                    <ul class="description">
                        <li>Led development of NECE (Narrative Event Chain Extraction) toolkit, building first online interactive system dedicated to narrative event chain analysis with 10% performance improvement.</li>
                        <li>Proposed first human-reaction based model to evaluate dialogue engagingness, surpassing existing metrics in 4 of 5 benchmarks.</li>
                        <li>Applied knowledge distillation and attention head pruning to reduce toxicity-level of GPT2 and Blenderbot, reducing memory by 50% and inference time by 60%.</li>
                    </ul>
                </div>
            </div>
        </section>

        <section id="projects" class="section section-alt">
            <div class="container">
                <h2>Selected Projects & Publications</h2>
                <div class="projects-grid">
                    <div class="project-card">
                        <h3>Dr. SoW: Preference Tuning</h3>
                        <p class="project-description">Novel method for automatic preference annotation using density ratio of strong-over-weak LLMs, significantly reducing human labeling costs in LLM alignment. (arXiv 2025)</p>
                        <div class="project-links">
                            <a href="https://scholar.google.com/citations?user=ohsEWqsAAAAJ&hl=en" target="_blank">Paper</a>
                        </div>
                    </div>
                    <div class="project-card">
                        <h3>Inference-Time Scaling via Particle Methods</h3>
                        <p class="project-description">Particle-based Monte Carlo sampling algorithm for efficient LLM inference-time compute scaling, achieving SOTA accuracy-compute tradeoffs. (NeurIPS 2024)</p>
                        <div class="project-links">
                            <a href="https://scholar.google.com/citations?user=ohsEWqsAAAAJ&hl=en" target="_blank">Paper</a>
                        </div>
                    </div>
                    <div class="project-card">
                        <h3>Merlinite-7B-PT</h3>
                        <p class="project-description">First preference-aligned LLM from InstructLab project, achieving GPT-3.5 level performance using only 48k AI-generated preference pairs. Released open-source under Apache 2.0.</p>
                        <div class="project-links">
                            <a href="https://huggingface.co/" target="_blank">HuggingFace</a>
                        </div>
                    </div>
                    <div class="project-card">
                        <h3>NECE: Narrative Event Chain Extraction</h3>
                        <p class="project-description">First online interactive system for narrative event chain analysis, combining BookNLP and AllenNLP with novel algorithms for character extraction and salient event identification.</p>
                        <div class="project-links">
                            <a href="https://scholar.google.com/citations?user=ohsEWqsAAAAJ&hl=en" target="_blank">Website</a>
                        </div>
                    </div>
                    <div class="project-card">
                        <h3>BRAIn: Bayesian Preference Alignment</h3>
                        <p class="project-description">Bayesian framework for LLM preference alignment adopted by Nvidia's Nemotron and IBM's Granite models. Reward-conditioned amortized inference for natural language generation. (ICML 2024)</p>
                        <div class="project-links">
                            <a href="https://scholar.google.com/citations?user=ohsEWqsAAAAJ&hl=en" target="_blank">Paper</a>
                        </div>
                    </div>
                    <div class="project-card">
                        <h3>EnDex: Dialogue Engagingness Evaluation</h3>
                        <p class="project-description">First human-reaction based model for evaluating dialogue engagingness at scale, trained on large Reddit corpus using distant-supervision framework. (EMNLP 2022)</p>
                        <div class="project-links">
                            <a href="https://scholar.google.com/citations?user=ohsEWqsAAAAJ&hl=en" target="_blank">Paper</a>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <section id="skills" class="section">
            <div class="container">
                <h2>Skills & Technologies</h2>
                <div class="skills-grid">
                    <div class="skill-category">
                        <h3>AI/ML Frameworks</h3>
                        <p>PyTorch, Spark, Pandas, Machine Learning, Deep Learning, NLP, LLM Alignment, Reinforcement Learning</p>
                    </div>
                    <div class="skill-category">
                        <h3>Developer Tools</h3>
                        <p>Python, Git, Docker, OpenShift, Cursor, ClaudeCode, SQLite</p>
                    </div>
                    <div class="skill-category">
                        <h3>Research Areas</h3>
                        <p>Inference-Time Scaling, Preference Alignment, Bayesian Inference, Reward Modeling, Dialogue Systems, Knowledge Distillation</p>
                    </div>
                    <div class="skill-category">
                        <h3>Specializations</h3>
                        <p>Large Language Models, Enterprise AI Agents, Post-Training Techniques, Document-Grounded Feedback, RAG Systems</p>
                    </div>
                </div>
            </div>
        </section>

        <section id="contact" class="section section-alt">
            <div class="container">
                <h2>Contact</h2>
                <p class="contact-text">
                    I'm always interested in new research collaborations and opportunities. Feel free to reach out to discuss AI/ML research, enterprise AI systems, or potential collaborations.
                </p>
                <div class="contact-info">
                    <p><strong>Email:</strong> <a href="mailto:gxu21@cs.ucla.edu">gxu21@cs.ucla.edu</a></p>
                    <p><strong>GitHub:</strong> <a href="https://github.com/gx-ai-architect" target="_blank">@gx-ai-architect</a></p>
                    <p><strong>LinkedIn:</strong> <a href="https://www.linkedin.com/in/gxxu/" target="_blank">linkedin.com/in/gxxu</a></p>
                    <p><strong>Google Scholar:</strong> <a href="https://scholar.google.com/citations?user=ohsEWqsAAAAJ&hl=en" target="_blank">Publications</a></p>
                </div>
            </div>
        </section>
    </main>

    <footer>
        <div class="container">
            <p>&copy; <span id="year"></span> Guangxuan (GX) Xu. All rights reserved.</p>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>
